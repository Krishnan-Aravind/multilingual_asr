{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker verification\n",
    "- Identify which layer has the highest performance. Plot this.\n",
    "- Use XLSR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "2.0.2\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "### import packages ###\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random \n",
    "import itertools \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to speaker .wav files\n",
    "AUDIO = \"./speaker_audio/\" \n",
    "UTTERANCES_PER_SPEAKER = 50 # number of audio utterances per speaker to gather embeddings from\n",
    "NUM_LAYERS = 24 # number of layers in ASR model\n",
    "NUM_SPEAKERS = 10 # number of unique speakers in training data\n",
    "NUM_SAMPLES = 2000 # number of u,v feature representations (elements in X) on which to train\n",
    "NUM_SAME = int(NUM_SAMPLES / 2)\n",
    "NUM_DIFF = int(NUM_SAMPLES / 2)\n",
    "\n",
    "# Path to ouput compressed data\n",
    "EMBEDDINGS = \"./audio_embeddings/\"\n",
    "\n",
    "# Load audio embeddings from XLS-R model [Babu et al., 2021] (torchaudio.pipelines.WAV2VEC2_XLSR_300M)\n",
    "with open(os.path.join(EMBEDDINGS, \"training_embeddings.pickle\"), \"rb\") as f:\n",
    "    training_embeddings = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(EMBEDDINGS, \"eval_embeddings.pickle\"), \"rb\") as f:\n",
    "    eval_embeddings = pickle.load(f)\n",
    "\n",
    "# Set up for plots\n",
    "sns.set(rc={'figure.figsize':(7,5)})\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "# fig, ax = plt.subplots(6,2,figsize=(11,8.5))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hear_audio(AUDIO_FILE):\n",
    "    audio, sr = torchaudio.load(AUDIO_FILE)\n",
    "    print(\"\\t\", end=\"\")\n",
    "    IPython.display.display(IPython.display.Audio(data=audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(sp_embeddings, sp, layer, index):\n",
    "    \"\"\"\n",
    "    Returns the torch.Size([1024]) utterance embedding.\n",
    "\n",
    "    param: sp_embeddings, list of sp_embeddings for all speakers, all layers, all utterances e.g. shape (10, 24, 50, 1024)\n",
    "    param: sp, int speaker\n",
    "    param: layer, int layer of the utterance embedding in range [0,NUM_LAYERS]\n",
    "    param: index, int representing which utterance in range [0,NUM_UTTERANCES)\n",
    "    \"\"\"\n",
    "    return sp_embeddings[sp][layer-1][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(data_object, filepath):\n",
    "    \"\"\"\n",
    "    Pickle a given data_object into the given filename, \n",
    "    saved to computer at filepath.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data_object, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that pickling worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 24, 50, 1024)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(training_embeddings))\n",
    "len(training_embeddings[0])\n",
    "np.asarray(training_embeddings).shape # 10 speakers, 24 layers, 50 utterances, 1024 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 24, 50, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(eval_embeddings))\n",
    "len(eval_embeddings)\n",
    "np.asarray(eval_embeddings).shape # 10 speakers, 24 layers, 50 utterances, 1024 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "[ 16.21347   -4.025654  12.113341 ... -10.794706   9.452261  43.080956]\n"
     ]
    }
   ],
   "source": [
    "print(eval_embeddings[0][23][0].shape)\n",
    "print(eval_embeddings[0][23][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker verification probing task with linear classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build X, y from training embeddings\n",
    "- X matrix of computed {u,v} vectors\n",
    "- y vector of labels {1,0} for {same,different}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample_indices(NUM_SAMPLES, UTTERANCES_PER_SPEAKER):\n",
    "\n",
    "    # TODO: make this cleaner and integrated with the NUM_UTTERANCES constant for easy changing/testing\n",
    "    # TODO: increase more samples (various ways, but as long as there are more samples)\n",
    "\n",
    "    # choose from 10 speakers x 50 utterances\n",
    "    # 500 pairs of same, 500 pairs of diff\n",
    "\n",
    "    print(\"NUM_SAMPLES:\", NUM_SAMPLES)\n",
    "    print(f\"\\tNUM_SAME = {NUM_SAME}\\n\\tNUM_DIFF = {NUM_DIFF}\")\n",
    "\n",
    "\n",
    "    # For same:\n",
    "    # 50 pairs per speaker 0-9 = 50*10 = 500\n",
    "    # within the 50 pairs: 0-49, 1-48, ... 24-25, 25-\n",
    "    # num_utts_per_speaker = int((NUM_SAMPLES/2) / NUM_SPEAKERS)\n",
    "    num_utts_per_speaker = UTTERANCES_PER_SPEAKER\n",
    "    print(num_utts_per_speaker)\n",
    "    # assert num_utts_per_speaker * NUM_SPEAKERS == NUM_SAMPLES/2, f\"Choose a value for NUM_SAMPLES/2 that is divisible by NUM_SPEAKERS, which is currently set to ({NUM_SPEAKERS}) \"\n",
    "\n",
    "    indices = [i for i in range(num_utts_per_speaker)]\n",
    "    pairs = list(itertools.combinations(indices, 2))\n",
    "    random.shuffle(pairs)\n",
    "    num_selected_per_speaker = int(NUM_SAME / NUM_SPEAKERS)\n",
    "    assert num_selected_per_speaker * NUM_SPEAKERS == NUM_SAME\n",
    "    selected = pairs[0:num_selected_per_speaker]\n",
    "\n",
    "    print(\"indices:\", indices)\n",
    "\n",
    "    # indexed_audio_same = [selected for sp in range(10)]\n",
    "    indexed_audio_same = []\n",
    "    for sp in range(NUM_SPEAKERS):\n",
    "        for pair in selected:\n",
    "            sp_audio_u = (sp, pair[0])\n",
    "            sp_audio_v = (sp, pair[1])\n",
    "            new_pair = (sp_audio_u, sp_audio_v)\n",
    "            indexed_audio_same.append(new_pair)\n",
    "\n",
    "    print(\"indexed_audio_same shape:\", np.asarray(indexed_audio_same).shape) # (500, 2, 2) = 500 pairs of [[sp, audio], [sp, audio]] indices\n",
    "    # print(indexed_audio_same[0:5])\n",
    "\n",
    "    # For diff:\n",
    "    # 1000 utts / 10 speakers = 100 utts per speaker, to make 500 pairs\n",
    "    # 100 utts / 9 other speakers = ~ group 11 utts with each other speaker (or select randomly)\n",
    "\n",
    "    indices_per_sp = np.tile(indices, 2) # 50 utts x 2 = 100 utts\n",
    "    print(\"indices_per_sp:\", indices_per_sp)\n",
    "\n",
    "    sp_audio_tuples = []\n",
    "    for sp in range(NUM_SPEAKERS):\n",
    "        for audio in indices_per_sp:\n",
    "            sp_audio = (sp, audio)\n",
    "            sp_audio_tuples.append(sp_audio)\n",
    "\n",
    "    pairs_diff = list(itertools.combinations(sp_audio_tuples, 2))\n",
    "    random.shuffle(pairs_diff)\n",
    "    selected = pairs_diff[0:NUM_DIFF]\n",
    "\n",
    "    indexed_audio_diff = selected\n",
    "    print(\"indexed_audio_diff shape:\", np.asarray(indexed_audio_diff).shape) # (500, 2, 2) = 500 pairs of [[sp_u, audio], [sp_v, audio]] indices\n",
    "    print(indexed_audio_diff[0:5])\n",
    "    print(indexed_audio_diff[6])\n",
    "\n",
    "    return (indexed_audio_same, indexed_audio_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(indexed_audio_same_diff, embeddings, NUM_SAMPLES, NUM_LAYERS):\n",
    "    \n",
    "    indexed_audio_same, indexed_audio_diff = indexed_audio_same_diff\n",
    "    \n",
    "    X_all_layers = []\n",
    "\n",
    "    for layer in range(NUM_LAYERS):\n",
    "        X = []\n",
    "        for match_type in [indexed_audio_same, indexed_audio_diff]:\n",
    "            for pair in match_type:\n",
    "                sp_u = pair[0][0]\n",
    "                sp_v = pair[1][0]\n",
    "                audio_u = pair[0][1]\n",
    "                audio_v = pair[1][1]\n",
    "                u = get_feature_vector(embeddings, sp_u, layer, audio_u)\n",
    "                v = get_feature_vector(embeddings, sp_v, layer, audio_v)\n",
    "                \n",
    "                # concatenate u and v vectors: [|u-v| + (u ⊙ v)]\n",
    "                concat = np.concatenate([abs(np.subtract(u, v)), np.multiply(u, v)])\n",
    "\n",
    "                X.append(concat)\n",
    "        \n",
    "        X_all_layers.append(X)\n",
    "\n",
    "    y = np.repeat([1,0], int(NUM_SAMPLES/2)) # 500 same 1, 500 diff 0\n",
    "\n",
    "    print(np.asarray(X_all_layers).shape)\n",
    "    print(np.asarray(X_all_layers[0]).shape)\n",
    "    assert np.asarray(X_all_layers[0]).shape[0] == NUM_SAMPLES\n",
    "    print(f\"NUM_SAMPLES ({NUM_SAMPLES}) == num elements in each X matrix ? {NUM_SAMPLES==np.asarray(X_all_layers[0]).shape[0]}\")\n",
    "    print(len(y))\n",
    "\n",
    "    print(type(X[0]), X[0], X[0].shape, sep=\"\\n\")\n",
    "\n",
    "    return X_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_new(indexed_audio_same_diff, embeddings, NUM_SAMPLES, NUM_LAYERS):\n",
    "    \n",
    "    indexed_audio_same, indexed_audio_diff = indexed_audio_same_diff\n",
    "    \n",
    "    X_all_layers = []\n",
    "\n",
    "    for layer in range(NUM_LAYERS):\n",
    "        X = []\n",
    "        for match_type in [indexed_audio_same, indexed_audio_diff]:\n",
    "            for pair in match_type:\n",
    "                sp_u = pair[0][0]\n",
    "                sp_v = pair[1][0]\n",
    "                audio_u = pair[0][1]\n",
    "                audio_v = pair[1][1]\n",
    "                u = get_feature_vector(embeddings, sp_u, layer, audio_u)\n",
    "                v = get_feature_vector(embeddings, sp_v, layer, audio_v)\n",
    "                \n",
    "                # concatenate u and v vectors: [|u-v| + (u ⊙ v)]\n",
    "                concat = np.concatenate([u,v])\n",
    "\n",
    "                X.append(concat)\n",
    "        \n",
    "        X_all_layers.append(X)\n",
    "\n",
    "    y = np.repeat([1,0], int(NUM_SAMPLES/2)) # 500 same 1, 500 diff 0\n",
    "\n",
    "    print(np.asarray(X_all_layers).shape)\n",
    "    print(np.asarray(X_all_layers[0]).shape)\n",
    "    assert np.asarray(X_all_layers[0]).shape[0] == NUM_SAMPLES\n",
    "    print(f\"NUM_SAMPLES ({NUM_SAMPLES}) == num elements in each X matrix ? {NUM_SAMPLES==np.asarray(X_all_layers[0]).shape[0]}\")\n",
    "    print(len(y))\n",
    "\n",
    "    print(type(X[0]), X[0], X[0].shape, sep=\"\\n\")\n",
    "\n",
    "    return X_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_concat(indexed_audio_same_diff, embeddings, NUM_SAMPLES, NUM_LAYERS):\n",
    "    \n",
    "    indexed_audio_same, indexed_audio_diff = indexed_audio_same_diff\n",
    "    \n",
    "    X_all_layers = []\n",
    "\n",
    "    for layer in range(NUM_LAYERS):\n",
    "        X = []\n",
    "        for match_type in [indexed_audio_same, indexed_audio_diff]:\n",
    "            for pair in match_type:\n",
    "                sp_u = pair[0][0]\n",
    "                sp_v = pair[1][0]\n",
    "                audio_u = pair[0][1]\n",
    "                audio_v = pair[1][1]\n",
    "                u = get_feature_vector(embeddings, sp_u, layer, audio_u)\n",
    "                v = get_feature_vector(embeddings, sp_v, layer, audio_v)\n",
    "                \n",
    "                # concatenate u and v vectors: [|u-v| + (u ⊙ v)]\n",
    "                concat = np.concatenate([u, v, abs(np.subtract(u, v)), np.multiply(u, v)])\n",
    "\n",
    "                X.append(concat)\n",
    "        \n",
    "        X_all_layers.append(X)\n",
    "\n",
    "    y = np.repeat([1,0], int(NUM_SAMPLES/2)) # 500 same 1, 500 diff 0\n",
    "\n",
    "    print(np.asarray(X_all_layers).shape)\n",
    "    print(np.asarray(X_all_layers[0]).shape)\n",
    "    assert np.asarray(X_all_layers[0]).shape[0] == NUM_SAMPLES\n",
    "    print(f\"NUM_SAMPLES ({NUM_SAMPLES}) == num elements in each X matrix ? {NUM_SAMPLES==np.asarray(X_all_layers[0]).shape[0]}\")\n",
    "    print(len(y))\n",
    "\n",
    "    print(type(X[0]), X[0], X[0].shape, sep=\"\\n\")\n",
    "\n",
    "    return X_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_SAMPLES: 2000\n",
      "\tNUM_SAME = 1000\n",
      "\tNUM_DIFF = 1000\n",
      "50\n",
      "indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "indexed_audio_same shape: (1000, 2, 2)\n",
      "indices_per_sp: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21\n",
      " 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n",
      " 46 47 48 49]\n",
      "indexed_audio_diff shape: (1000, 2, 2)\n",
      "[((1, 14), (9, 23)), ((1, 15), (7, 16)), ((3, 17), (8, 43)), ((3, 22), (9, 27)), ((0, 26), (3, 26))]\n",
      "((0, 19), (7, 26))\n"
     ]
    }
   ],
   "source": [
    "indexed_audio_same_diff = build_sample_indices(NUM_SAMPLES, UTTERANCES_PER_SPEAKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 2000, 2048)\n",
      "(2000, 2048)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[3.4801292e-01 1.7270350e+00 3.0420547e+00 ... 2.0204884e+02 9.7382488e+00\n",
      " 1.4272496e+03]\n",
      "(2048,)\n",
      "len(y):2000\n"
     ]
    }
   ],
   "source": [
    "# Build training X\n",
    "X_all_layers = build_X(indexed_audio_same_diff, training_embeddings, NUM_SAMPLES, NUM_LAYERS)\n",
    "y = np.repeat([1,0], int(NUM_SAMPLES/2)) \n",
    "\n",
    "print(f\"len(y): {len(y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build X test set from EVALUATION embeddings\n",
    "- X matrix of computed {u,v} vectors\n",
    "- y vector of labels {1,0} for {same,different} is the same as for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 2000, 2048)\n",
      "(2000, 2048)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[   5.236721     3.0003624    6.4331274 ...  184.84256     14.38203\n",
      " 1428.4341   ]\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "# Build evaluation X\n",
    "X_test = build_X(indexed_audio_same_diff, eval_embeddings, NUM_SAMPLES, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9955\n",
      "clf.score layer 1: 0.588\n",
      "0.991\n",
      "clf.score layer 2: 0.69\n",
      "0.9925\n",
      "clf.score layer 3: 0.636\n",
      "0.9955\n",
      "clf.score layer 4: 0.6265\n",
      "0.995\n",
      "clf.score layer 5: 0.697\n",
      "0.9955\n",
      "clf.score layer 6: 0.7385\n",
      "0.995\n",
      "clf.score layer 7: 0.734\n",
      "0.994\n",
      "clf.score layer 8: 0.7575\n",
      "0.9935\n",
      "clf.score layer 9: 0.72\n",
      "0.9925\n",
      "clf.score layer 10: 0.684\n",
      "0.9925\n",
      "clf.score layer 11: 0.7185\n",
      "0.993\n",
      "clf.score layer 12: 0.659\n",
      "0.9955\n",
      "clf.score layer 13: 0.6765\n",
      "0.9955\n",
      "clf.score layer 14: 0.6735\n",
      "0.995\n",
      "clf.score layer 15: 0.6905\n",
      "0.9955\n",
      "clf.score layer 16: 0.614\n",
      "0.9955\n",
      "clf.score layer 17: 0.614\n",
      "0.9955\n",
      "clf.score layer 18: 0.619\n",
      "0.9955\n",
      "clf.score layer 19: 0.7415\n",
      "0.9955\n",
      "clf.score layer 20: 0.669\n",
      "0.9955\n",
      "clf.score layer 21: 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9955\n",
      "clf.score layer 22: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n",
      "clf.score layer 23: 0.649\n",
      "0.957\n",
      "clf.score layer 24: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# X, y = \n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :])\n",
    "clf.score(X, y)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tips to address convergence warning:\n",
    "- try different solvers: https://scikit-learn.org/stable/modules/linear_model.html#solvers (default is lbfgs)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "layer_accuracy = []\n",
    "\n",
    "# Q: max_iter? A: solved by using a different optimization algorithm\n",
    "# clf = LogisticRegression(solver=\"liblinear\",random_state=0, max_iter=10000).fit(X, y)\n",
    "# print(\"clf.score(X, y):\", clf.score(X, y))\n",
    "\n",
    "# print(clf.predict(X[0:5]))\n",
    "# print(clf.predict(X[900:905]))\n",
    "# print(clf.predict_proba(X[900:905]))\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "    clf = LogisticRegression(solver=\"lbfgs\",random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(layer_accuracy) # layer 4, when num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try reducing iterations --> all convergence warnings\n",
    "\n",
    "layer_accuracy_5 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "    clf = LogisticRegression(solver=\"lbfgs\",random_state=0, max_iter=100).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_5.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer accuracy using LogisticRegression liblinear:\n",
      "0.9955\n",
      "clf.score layer 1: 0.585\n",
      "0.991\n",
      "clf.score layer 2: 0.688\n",
      "0.9925\n",
      "clf.score layer 3: 0.6365\n",
      "0.9955\n",
      "clf.score layer 4: 0.626\n",
      "0.995\n",
      "clf.score layer 5: 0.697\n",
      "0.9955\n",
      "clf.score layer 6: 0.74\n",
      "0.995\n",
      "clf.score layer 7: 0.732\n",
      "0.994\n",
      "clf.score layer 8: 0.7595\n",
      "0.9935\n",
      "clf.score layer 9: 0.7205\n",
      "0.9935\n",
      "clf.score layer 10: 0.6855\n",
      "0.9925\n",
      "clf.score layer 11: 0.7185\n",
      "0.993\n",
      "clf.score layer 12: 0.6615\n",
      "0.995\n",
      "clf.score layer 13: 0.6775\n",
      "0.9955\n",
      "clf.score layer 14: 0.6755\n",
      "0.995\n",
      "clf.score layer 15: 0.693\n",
      "0.9955\n",
      "clf.score layer 16: 0.614\n",
      "0.9955\n",
      "clf.score layer 17: 0.6155\n",
      "0.9955\n",
      "clf.score layer 18: 0.614\n",
      "0.9955\n",
      "clf.score layer 19: 0.7395\n",
      "0.9955\n",
      "clf.score layer 20: 0.6705\n",
      "0.9955\n",
      "clf.score layer 21: 0.6475\n",
      "0.9955\n",
      "clf.score layer 22: 0.6805\n",
      "0.9955\n",
      "clf.score layer 23: 0.629\n",
      "0.9955\n",
      "clf.score layer 24: 0.5455\n"
     ]
    }
   ],
   "source": [
    "layer_accuracy_3 = []\n",
    "\n",
    "print(f\"Layer accuracy using LogisticRegression liblinear:\")\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "    clf = LogisticRegression(solver=\"liblinear\",random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_3.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "clf.score layer 1: 0.5\n",
      "0.9\n",
      "clf.score layer 2: 0.787\n",
      "0.7855\n",
      "clf.score layer 3: 0.5925\n",
      "0.928\n",
      "clf.score layer 4: 0.8765\n",
      "0.9125\n",
      "clf.score layer 5: 0.8545\n",
      "0.901\n",
      "clf.score layer 6: 0.7885\n",
      "0.861\n",
      "clf.score layer 7: 0.742\n",
      "0.832\n",
      "clf.score layer 8: 0.742\n",
      "0.8175\n",
      "clf.score layer 9: 0.7145\n",
      "0.8\n",
      "clf.score layer 10: 0.6995\n",
      "0.7865\n",
      "clf.score layer 11: 0.7\n",
      "0.502\n",
      "clf.score layer 12: 0.5055\n",
      "0.7435\n",
      "clf.score layer 13: 0.6835\n",
      "0.5\n",
      "clf.score layer 14: 0.5\n",
      "0.763\n",
      "clf.score layer 15: 0.661\n",
      "0.5\n",
      "clf.score layer 16: 0.5\n",
      "0.5\n",
      "clf.score layer 17: 0.5\n",
      "0.561\n",
      "clf.score layer 18: 0.5155\n",
      "0.8115\n",
      "clf.score layer 19: 0.718\n",
      "0.6815\n",
      "clf.score layer 20: 0.5715\n",
      "0.511\n",
      "clf.score layer 21: 0.512\n",
      "0.5\n",
      "clf.score layer 22: 0.5\n",
      "0.5\n",
      "clf.score layer 23: 0.5\n",
      "0.5\n",
      "clf.score layer 24: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer_accuracy_2 = []\n",
    "\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    # clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    clf = SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", random_state=0).fit(X,y) \n",
    "    # clf = LogisticRegression(random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_2.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(layer_accuracy_2) # layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "clf.score layer 1: 0.5\n",
      "0.962\n",
      "clf.score layer 2: 0.8325\n",
      "0.995\n",
      "clf.score layer 3: 0.7515\n",
      "0.9525\n",
      "clf.score layer 4: 0.676\n",
      "0.995\n",
      "clf.score layer 5: 0.778\n",
      "0.9825\n",
      "clf.score layer 6: 0.776\n",
      "0.9805\n",
      "clf.score layer 7: 0.862\n",
      "0.9835\n",
      "clf.score layer 8: 0.7645\n",
      "0.965\n",
      "clf.score layer 9: 0.745\n",
      "0.8915\n",
      "clf.score layer 10: 0.619\n",
      "0.94\n",
      "clf.score layer 11: 0.8255\n",
      "0.8885\n",
      "clf.score layer 12: 0.802\n",
      "0.9095\n",
      "clf.score layer 13: 0.7955\n",
      "0.932\n",
      "clf.score layer 14: 0.73\n",
      "0.931\n",
      "clf.score layer 15: 0.7145\n",
      "0.904\n",
      "clf.score layer 16: 0.7805\n",
      "0.948\n",
      "clf.score layer 17: 0.7995\n",
      "0.93\n",
      "clf.score layer 18: 0.593\n",
      "0.883\n",
      "clf.score layer 19: 0.864\n",
      "0.9435\n",
      "clf.score layer 20: 0.8675\n",
      "0.9205\n",
      "clf.score layer 21: 0.721\n",
      "0.9415\n",
      "clf.score layer 22: 0.858\n",
      "0.5\n",
      "clf.score layer 23: 0.5\n",
      "0.54\n",
      "clf.score layer 24: 0.496\n"
     ]
    }
   ],
   "source": [
    "layer_accuracy_4 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_4.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 2000, 2048)\n",
      "(2000, 2048)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[ 21.701027   -4.839901   -1.4761006 ... -16.792784    5.7009068\n",
      "  36.963272 ]\n",
      "(2048,)\n",
      "(24, 2000, 2048)\n",
      "(2000, 2048)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[ 18.729002   -1.5524691   5.3402205 ... -12.456214    1.8552047\n",
      "  40.80391  ]\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "X_all_layers = build_X_new(indexed_audio_same_diff, training_embeddings, NUM_SAMPLES, NUM_LAYERS)\n",
    "X_test = build_X_new(indexed_audio_same_diff, eval_embeddings, NUM_SAMPLES, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9955\n",
      "clf.score layer 1: 0.5235\n",
      "0.9555\n",
      "clf.score layer 2: 0.5105\n",
      "0.971\n",
      "clf.score layer 3: 0.5155\n",
      "0.9765\n",
      "clf.score layer 4: 0.529\n",
      "0.9845\n",
      "clf.score layer 5: 0.521\n",
      "0.9895\n",
      "clf.score layer 6: 0.511\n",
      "0.985\n",
      "clf.score layer 7: 0.4975\n",
      "0.981\n",
      "clf.score layer 8: 0.482\n",
      "0.9825\n",
      "clf.score layer 9: 0.5175\n",
      "0.9795\n",
      "clf.score layer 10: 0.5265\n",
      "0.975\n",
      "clf.score layer 11: 0.5315\n",
      "0.9805\n",
      "clf.score layer 12: 0.541\n",
      "0.987\n",
      "clf.score layer 13: 0.538\n",
      "0.9845\n",
      "clf.score layer 14: 0.5375\n",
      "0.989\n",
      "clf.score layer 15: 0.537\n",
      "0.989\n",
      "clf.score layer 16: 0.548\n",
      "0.9885\n",
      "clf.score layer 17: 0.557\n",
      "0.9875\n",
      "clf.score layer 18: 0.553\n",
      "0.9925\n",
      "clf.score layer 19: 0.5475\n",
      "0.9935\n",
      "clf.score layer 20: 0.5675\n",
      "0.994\n",
      "clf.score layer 21: 0.55\n",
      "0.995\n",
      "clf.score layer 22: 0.555\n",
      "0.995\n",
      "clf.score layer 23: 0.558\n",
      "0.9955\n",
      "clf.score layer 24: 0.5175\n"
     ]
    }
   ],
   "source": [
    "layer_accuracy_6 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    # clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    # clf = SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", random_state=0).fit(X,y) \n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_6.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5345\n",
      "clf.score layer 1: 0.499\n",
      "0.9485\n",
      "clf.score layer 2: 0.6395\n",
      "0.9415\n",
      "clf.score layer 3: 0.597\n",
      "0.9565\n",
      "clf.score layer 4: 0.568\n",
      "0.9575\n",
      "clf.score layer 5: 0.563\n",
      "0.991\n",
      "clf.score layer 6: 0.5075\n",
      "0.9885\n",
      "clf.score layer 7: 0.583\n",
      "0.9545\n",
      "clf.score layer 8: 0.491\n",
      "0.981\n",
      "clf.score layer 9: 0.552\n",
      "0.949\n",
      "clf.score layer 10: 0.561\n",
      "0.9635\n",
      "clf.score layer 11: 0.575\n",
      "0.978\n",
      "clf.score layer 12: 0.5755\n",
      "0.8145\n",
      "clf.score layer 13: 0.5055\n",
      "0.9815\n",
      "clf.score layer 14: 0.562\n",
      "0.963\n",
      "clf.score layer 15: 0.52\n",
      "0.977\n",
      "clf.score layer 16: 0.553\n",
      "0.984\n",
      "clf.score layer 17: 0.5825\n",
      "0.975\n",
      "clf.score layer 18: 0.5795\n",
      "0.9985\n",
      "clf.score layer 19: 0.5985\n",
      "0.9845\n",
      "clf.score layer 20: 0.481\n",
      "0.951\n",
      "clf.score layer 21: 0.4795\n",
      "0.9535\n",
      "clf.score layer 22: 0.58\n",
      "0.5035\n",
      "clf.score layer 23: 0.501\n",
      "0.5\n",
      "clf.score layer 24: 0.5\n"
     ]
    }
   ],
   "source": [
    "layer_accuracy_6 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    # clf = SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", random_state=0).fit(X,y) \n",
    "    # clf = LogisticRegression(random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_6.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 2000, 4096)\n",
      "(2000, 4096)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[  21.701027    -4.839901    -1.4761006 ...  202.04884      9.738249\n",
      " 1427.2496   ]\n",
      "(4096,)\n",
      "(24, 2000, 4096)\n",
      "(2000, 4096)\n",
      "NUM_SAMPLES (2000) == num elements in each X matrix ? True\n",
      "2000\n",
      "<class 'numpy.ndarray'>\n",
      "[  18.729002    -1.5524691    5.3402205 ...  184.84256     14.38203\n",
      " 1428.4341   ]\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "X_all_layers = build_X_concat(indexed_audio_same_diff, training_embeddings, NUM_SAMPLES, NUM_LAYERS)\n",
    "X_test = build_X_concat(indexed_audio_same_diff, eval_embeddings, NUM_SAMPLES, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_accuracy_6 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    # clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    # clf = SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", random_state=0).fit(X,y) \n",
    "    clf = LogisticRegression(solver=\"liblinear\", random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_6.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_accuracy_6 = []\n",
    "\n",
    "for i, X in enumerate(X_all_layers):\n",
    "    # train linear classifier. Fit the model according to the given *training* data.\n",
    "\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(X, y)\n",
    "    # clf = SGDClassifier(loss=\"squared_hinge\", penalty=\"l2\", random_state=0).fit(X,y) \n",
    "    # clf = LogisticRegression(random_state=0, max_iter=10000).fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "\n",
    "    # Return the mean accuracy on the given *test* data and labels.\n",
    "    score = clf.score(X_test[i], y)\n",
    "    layer_accuracy_6.append(score)\n",
    "    print(f\"clf.score layer {i+1}:\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 1 1]\n",
      "0.6615\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver=\"liblinear\",random_state=0, max_iter=10000).fit(X_all_layers[12-1], y)\n",
    "layer_predictions = clf.predict(X_test[12-1])\n",
    "print(layer_predictions)\n",
    "print(clf.score(X_test[12-1], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2, 2)\n",
      "(2000, 2) [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (4, 4), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (5, 5), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (6, 6), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (7, 7), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (8, 8), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (9, 9), (1, 9), (1, 7), (3, 8), (3, 9), (0, 3), (1, 2), (0, 7), (8, 9), (0, 6), (4, 5), (3, 6), (2, 3), (4, 7), (2, 3), (0, 1), (5, 9), (1, 5), (3, 5), (5, 5), (0, 3), (1, 6), (4, 7), (2, 3), (3, 5), (6, 8), (2, 5), (6, 9), (3, 5), (2, 9), (0, 9), (2, 3), (2, 7), (0, 1), (1, 9), (4, 7), (0, 4), (2, 9), (2, 5), (8, 8), (1, 6), (1, 9), (1, 4), (0, 8), (8, 9), (0, 3), (1, 8), (3, 7), (0, 9), (0, 6), (4, 5), (4, 4), (1, 4), (3, 6), (1, 9), (5, 8), (2, 7), (3, 4), (6, 8), (5, 5), (6, 7), (1, 4), (1, 1), (1, 5), (0, 1), (2, 4), (0, 5), (2, 5), (4, 6), (1, 7), (1, 6), (6, 8), (6, 6), (7, 9), (1, 2), (3, 9), (1, 5), (0, 6), (1, 5), (2, 4), (5, 6), (3, 6), (2, 7), (2, 4), (0, 0), (5, 8), (2, 6), (8, 9), (0, 6), (1, 7), (3, 9), (5, 5), (7, 9), (2, 9), (2, 4), (1, 5), (3, 3), (5, 8), (4, 8), (2, 9), (4, 6), (6, 8), (7, 9), (1, 7), (2, 3), (5, 9), (6, 6), (2, 6), (4, 9), (0, 3), (3, 7), (4, 9), (5, 7), (0, 3), (0, 2), (0, 4), (1, 5), (4, 7), (7, 9), (0, 7), (1, 5), (2, 5), (0, 3), (0, 8), (4, 9), (0, 7), (1, 8), (1, 5), (3, 6), (0, 0), (5, 6), (0, 2), (0, 6), (2, 2), (2, 4), (1, 4), (4, 9), (0, 3), (0, 4), (3, 3), (9, 9), (3, 5), (4, 8), (3, 6), (2, 2), (0, 9), (7, 9), (7, 7), (3, 9), (3, 8), (3, 8), (4, 5), (3, 9), (0, 8), (0, 6), (6, 7), (1, 5), (0, 2), (2, 7), (3, 7), (1, 6), (7, 8), (2, 9), (4, 4), (1, 3), (4, 6), (0, 2), (3, 8), (0, 3), (2, 3), (0, 4), (1, 6), (3, 8), (3, 5), (2, 6), (7, 8), (2, 3), (1, 7), (4, 5), (3, 8), (1, 4), (1, 1), (4, 8), (0, 7), (3, 9), (1, 7), (8, 8), (6, 8), (4, 8), (7, 8), (1, 1), (6, 8), (2, 2), (1, 8), (5, 9), (6, 7), (3, 3), (0, 7), (2, 4), (0, 8), (3, 8), (4, 4), (5, 5), (3, 9), (4, 6), (5, 9), (3, 6), (0, 6), (0, 6), (4, 5), (1, 3), (1, 5), (3, 7), (1, 9), (6, 9), (0, 9), (0, 9), (5, 8), (0, 9), (1, 5), (2, 2), (2, 4), (3, 6), (1, 6), (0, 5), (4, 6), (0, 1), (1, 5), (7, 8), (1, 6), (3, 3), (7, 8), (1, 3), (2, 8), (0, 8), (0, 7), (3, 9), (5, 9), (3, 8), (3, 8), (3, 8), (2, 3), (1, 1), (1, 4), (3, 8), (0, 7), (0, 0), (0, 6), (2, 6), (3, 9), (2, 6), (8, 9), (3, 4), (1, 4), (1, 1), (0, 7), (2, 6), (9, 9), (3, 9), (1, 4), (0, 7), (2, 5), (2, 6), (0, 0), (3, 8), (1, 4), (1, 9), (3, 6), (4, 6), (3, 4), (5, 7), (4, 9), (7, 9), (2, 2), (3, 9), (2, 5), (3, 6), (1, 6), (3, 4), (1, 2), (2, 8), (7, 9), (1, 5), (8, 9), (0, 2), (1, 6), (7, 9), (2, 9), (3, 9), (3, 9), (3, 7), (1, 6), (0, 2), (1, 6), (2, 7), (1, 4), (1, 7), (2, 3), (0, 4), (4, 5), (6, 6), (2, 4), (0, 9), (9, 9), (2, 3), (7, 9), (1, 3), (7, 8), (4, 8), (2, 3), (7, 8), (3, 8), (1, 8), (2, 9), (0, 9), (2, 7), (0, 6), (1, 2), (3, 8), (1, 6), (6, 9), (0, 4), (5, 7), (4, 6), (1, 9), (4, 5), (0, 7), (1, 7), (5, 8), (1, 5), (0, 7), (7, 9), (3, 7), (2, 5), (2, 4), (5, 9), (4, 5), (3, 4), (0, 8), (3, 7), (5, 6), (4, 6), (5, 7), (0, 4), (4, 6), (7, 9), (0, 9), (2, 5), (7, 8), (1, 4), (8, 8), (3, 7), (0, 0), (2, 8), (0, 1), (4, 9), (7, 8), (7, 7), (3, 5), (2, 7), (7, 9), (3, 3), (3, 6), (7, 7), (1, 7), (5, 6), (2, 4), (1, 4), (1, 8), (3, 8), (2, 7), (6, 9), (0, 8), (3, 7), (8, 9), (4, 4), (6, 7), (8, 9), (4, 4), (5, 9), (4, 5), (2, 3), (1, 5), (0, 6), (0, 5), (2, 7), (1, 3), (0, 9), (1, 1), (1, 8), (2, 2), (2, 9), (7, 8), (0, 4), (4, 8), (2, 5), (4, 9), (5, 5), (1, 2), (6, 9), (0, 1), (3, 5), (0, 7), (5, 6), (1, 2), (2, 8), (0, 3), (6, 8), (5, 7), (2, 3), (2, 5), (4, 9), (1, 9), (4, 6), (7, 9), (0, 6), (0, 3), (1, 1), (6, 6), (0, 5), (6, 7), (6, 7), (7, 9), (2, 8), (2, 3), (3, 4), (6, 9), (0, 9), (6, 7), (2, 6), (2, 8), (3, 7), (0, 2), (5, 8), (2, 2), (4, 6), (3, 4), (2, 7), (0, 9), (0, 8), (6, 7), (1, 9), (3, 9), (0, 7), (4, 9), (0, 5), (3, 9), (2, 2), (1, 9), (0, 8), (2, 4), (3, 6), (0, 3), (0, 6), (1, 5), (0, 8), (2, 5), (0, 9), (3, 4), (4, 4), (3, 9), (4, 9), (7, 7), (1, 5), (2, 7), (4, 8), (4, 8), (2, 7), (2, 8), (7, 9), (1, 9), (1, 2), (2, 9), (2, 3), (1, 9), (6, 7), (9, 9), (1, 1), (1, 3), (4, 5), (1, 2), (1, 5), (0, 7), (0, 2), (4, 6), (7, 9), (1, 7), (3, 8), (4, 8), (2, 2), (6, 9), (5, 9), (5, 7), (2, 6), (5, 6), (1, 6), (1, 7), (3, 4), (0, 5), (6, 7), (5, 6), (7, 9), (2, 6), (1, 9), (3, 7), (0, 6), (6, 9), (1, 7), (5, 9), (2, 8), (4, 8), (1, 3), (0, 1), (3, 8), (0, 2), (5, 7), (2, 8), (5, 6), (4, 5), (7, 9), (3, 7), (3, 7), (1, 5), (2, 3), (0, 7), (2, 5), (6, 9), (6, 9), (5, 6), (0, 6), (0, 7), (1, 3), (7, 8), (3, 6), (1, 4), (3, 6), (0, 5), (0, 7), (0, 4), (3, 4), (5, 9), (2, 6), (0, 4), (1, 8), (1, 1), (5, 8), (6, 8), (4, 4), (3, 8), (2, 9), (1, 6), (2, 9), (3, 7), (0, 8), (4, 5), (0, 5), (0, 9), (2, 8), (1, 7), (3, 6), (0, 3), (0, 4), (1, 8), (3, 4), (2, 6), (5, 8), (4, 7), (3, 3), (4, 8), (0, 7), (9, 9), (0, 2), (2, 3), (6, 6), (0, 8), (1, 3), (0, 5), (4, 8), (1, 9), (0, 3), (3, 7), (1, 3), (3, 9), (1, 2), (0, 8), (1, 5), (2, 6), (3, 6), (3, 5), (3, 7), (5, 7), (3, 7), (3, 6), (1, 7), (7, 8), (4, 4), (0, 1), (7, 9), (4, 9), (1, 9), (0, 6), (5, 8), (1, 3), (4, 8), (4, 6), (3, 6), (9, 9), (1, 2), (2, 8), (3, 8), (0, 2), (8, 8), (0, 8), (2, 9), (1, 5), (3, 8), (0, 5), (4, 7), (3, 5), (7, 9), (3, 8), (7, 8), (0, 2), (0, 0), (4, 9), (7, 9), (1, 4), (7, 9), (0, 2), (0, 7), (4, 9), (2, 5), (4, 9), (2, 7), (2, 6), (0, 5), (4, 8), (2, 8), (3, 6), (4, 5), (1, 3), (2, 4), (1, 6), (1, 9), (6, 7), (4, 6), (3, 9), (3, 4), (4, 8), (6, 9), (3, 6), (4, 8), (3, 7), (2, 3), (0, 5), (6, 6), (3, 7), (1, 7), (4, 9), (1, 9), (5, 8), (0, 9), (0, 1), (5, 6), (3, 5), (2, 3), (2, 5), (4, 5), (5, 6), (1, 8), (1, 9), (0, 9), (0, 2), (4, 9), (2, 2), (1, 4), (6, 9), (0, 3), (2, 9), (3, 7), (6, 9), (1, 5), (7, 8), (2, 6), (2, 7), (5, 8), (0, 2), (5, 8), (0, 8), (6, 7), (5, 9), (4, 5), (0, 3), (1, 8), (7, 8), (0, 8), (4, 9), (4, 5), (6, 9), (0, 3), (7, 8), (3, 6), (1, 7), (3, 8), (8, 9), (0, 4), (3, 6), (1, 4), (0, 5), (1, 4), (1, 3), (4, 8), (3, 6), (1, 2), (2, 8), (0, 7), (1, 9), (3, 5), (5, 7), (6, 6), (0, 7), (4, 9), (7, 9), (6, 9), (3, 5), (1, 8), (0, 6), (1, 4), (0, 8), (0, 3), (2, 5), (3, 6), (4, 5), (1, 2), (3, 7), (2, 3), (4, 8), (2, 3), (4, 5), (0, 3), (2, 6), (0, 1), (5, 5), (0, 2), (6, 7), (1, 5), (3, 5), (3, 6), (1, 3), (3, 3), (0, 9), (6, 7), (1, 2), (9, 9), (0, 2), (5, 5), (4, 8), (2, 5), (3, 6), (2, 4), (0, 9), (5, 8), (7, 8), (6, 7), (2, 5), (4, 7), (3, 9), (8, 9), (1, 8), (4, 4), (4, 9), (0, 9), (0, 4), (1, 3), (2, 8), (0, 1), (5, 7), (0, 4), (3, 4), (8, 9), (3, 7), (4, 4), (6, 9), (1, 5), (1, 7), (4, 7), (0, 0), (1, 7), (1, 7), (7, 9), (4, 8), (1, 1), (3, 7), (2, 3), (3, 4), (1, 2), (0, 5), (1, 6), (3, 4), (2, 9), (2, 2), (1, 9), (1, 7), (1, 8), (1, 4), (1, 7), (3, 9), (1, 9), (0, 6), (0, 9), (5, 9), (3, 8), (4, 8), (4, 9), (0, 1), (3, 3), (6, 6), (2, 7), (1, 2), (2, 9), (1, 3), (2, 9), (4, 9), (6, 7), (1, 2), (2, 8), (7, 9), (6, 7), (0, 1), (4, 4), (1, 7), (5, 9), (8, 9), (3, 7), (0, 1), (2, 8), (0, 6), (0, 3), (2, 3), (1, 2), (5, 6), (4, 4), (8, 9), (1, 2), (4, 9), (4, 9), (0, 6), (8, 9), (3, 7), (1, 6), (0, 8), (2, 5), (4, 7), (0, 8), (7, 9), (6, 8), (8, 9), (5, 5), (8, 9), (0, 4), (4, 6), (7, 7), (6, 9), (1, 8), (3, 6), (6, 8), (1, 2), (0, 3), (0, 0), (7, 7), (9, 9), (5, 6), (0, 2), (5, 5), (3, 9), (4, 4), (4, 5), (3, 4), (4, 5), (9, 9), (2, 5), (1, 6), (0, 6), (2, 9), (3, 9), (0, 3), (0, 7), (0, 0), (0, 4), (3, 5), (1, 3), (2, 5), (3, 9), (0, 1), (4, 7), (0, 8), (0, 5), (5, 6), (0, 9), (0, 5), (8, 9), (0, 4), (3, 5), (1, 9), (2, 5), (8, 9), (6, 8), (1, 3), (4, 6), (4, 8), (1, 5), (2, 7), (7, 9), (3, 8), (4, 8), (5, 9), (2, 9), (2, 7), (1, 3), (2, 3), (1, 5), (5, 6), (7, 9), (4, 9), (6, 8), (5, 7), (4, 7), (5, 6), (6, 7), (2, 2), (1, 5), (3, 3), (7, 9), (0, 5), (0, 6), (0, 5), (0, 5), (7, 9), (3, 4), (0, 9), (2, 3), (0, 4), (5, 9), (3, 9), (5, 6), (4, 5), (0, 9), (3, 5), (0, 7), (3, 7), (7, 8), (5, 9), (1, 8), (2, 2), (4, 4), (1, 9), (5, 6), (3, 6), (2, 8), (6, 9), (2, 6), (5, 8), (5, 7), (1, 9), (1, 4), (7, 7), (3, 7), (1, 5), (8, 9), (0, 7), (1, 8), (1, 4), (0, 5), (5, 6), (4, 5), (4, 5), (2, 4), (6, 8), (1, 3), (1, 2), (4, 5), (1, 5), (4, 4), (7, 9), (1, 3), (9, 9), (0, 5), (1, 1), (1, 7), (1, 9), (4, 6), (2, 7), (0, 3), (2, 4), (2, 9), (0, 6), (1, 4), (0, 5), (3, 6), (1, 6), (0, 0)]\n",
      "(2000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(54.75, 0.5, 'speaker v')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexed_audio_same, indexed_audio_diff = indexed_audio_same_diff\n",
    "sp_indices = indexed_audio_same + indexed_audio_diff\n",
    "print(np.asarray(sp_indices).shape)\n",
    "sp_indices = [(sp[0][0],sp[1][0]) for sp in sp_indices]\n",
    "print(np.asarray(sp_indices).shape, sp_indices)\n",
    "print(np.asarray(layer_predictions).shape)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"predictions\"] = layer_predictions\n",
    "df[[\"x\", \"y\"]] = sp_indices\n",
    "heatmap = df.pivot_table(index='x', columns='y', values='predictions')\n",
    "\n",
    "sns.heatmap(heatmap)\n",
    "plt.xlabel(\"speaker u\")\n",
    "plt.ylabel(\"speaker v\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-wise analysis / plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n",
      "[0.588  0.69   0.636  0.6265 0.697  0.7385 0.734  0.7575 0.72   0.684\n",
      " 0.7185 0.659  0.6765 0.6735 0.6905 0.614  0.614  0.619  0.7415 0.669\n",
      " 0.6465 0.669  0.649  0.562 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(layer_accuracy).shape)\n",
    "\n",
    "print(np.asarray(layer_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
