{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcfcd749-e5aa-449a-9058-7552664ad1da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_12488/3566876723.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for wav in tqdm.tqdm_notebook(os.listdir(f'{root_dir}/{speaker.speaker}/wav/')):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c46faeec3074efb9b9c0a9e4d676eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/2 [00:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m transcript_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;241m.\u001b[39mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/transcript/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwav\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#Reading transcripts and pairs. Note the forced resampling in librosa.load()\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m array,sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m transcript\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(transcript_path)\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     39\u001b[0m data_dict\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m : array, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m'\u001b[39m:sampling_rate},\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m : transcript,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccent\u001b[39m\u001b[38;5;124m'\u001b[39m : speaker\u001b[38;5;241m.\u001b[39maccent,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m: speaker\u001b[38;5;241m.\u001b[39mspeaker}\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/librosa/core/audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    172\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/librosa/core/audio.py:604\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m soxr\u001b[38;5;241m.\u001b[39mresample(y\u001b[38;5;241m.\u001b[39mT, orig_sr, target_sr, quality\u001b[38;5;241m=\u001b[39mres_type)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[1;32m    607\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, n_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/resampy/core.py:120\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m x_2d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m0\u001b[39m, axis)\u001b[38;5;241m.\u001b[39mreshape((x\u001b[38;5;241m.\u001b[39mshape[axis], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    119\u001b[0m y_2d \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m0\u001b[39m, axis)\u001b[38;5;241m.\u001b[39mreshape((y\u001b[38;5;241m.\u001b[39mshape[axis], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 120\u001b[0m \u001b[43mresample_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterp_win\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterp_delta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import re\n",
    "import tqdm\n",
    "import librosa\n",
    "from datasets import Dataset, load_metric\n",
    "import json\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from src.data_util import DataCollatorCTCWithPadding\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "root_dir = '/data/users/akrishnan/multilingual_asr/corpora/l2arctic'\n",
    "speaker_info = pd.read_csv(f'{root_dir}/speaker_info.csv',sep='|')\n",
    "speaker_info.columns = ['speaker','gender','accent','wav_files','annotations']\n",
    "#SUBSAMPLING MODEL\n",
    "# speaker_info = speaker_info[speaker_info[\"speaker\"].isin(['YBAA','ZHAA','LXC','BWC','HQTV','THV','HJK','YKWK'])]\n",
    "speaker_info = speaker_info[speaker_info[\"speaker\"].isin(['YBAA','BDL'])]\n",
    "\n",
    "\n",
    "speaker_files = {}\n",
    "arctic_data = pd.DataFrame(['path','sentence','name','gender','accent','speaker'])\n",
    "data_list=[]\n",
    "for speaker in tqdm.tqdm(speaker_info.itertuples(index=False),total=len(speaker_info)):\n",
    "    wav_transcript_dict = {}\n",
    "    for wav in tqdm.tqdm_notebook(os.listdir(f'{root_dir}/{speaker.speaker}/wav/')):\n",
    "        #Getting full paths for the wav transcript pairs\n",
    "        wav_path = f\"{root_dir}/{speaker.speaker}/wav/{wav}\"\n",
    "        transcript_path = f\"{root_dir}/{speaker.speaker}/transcript/{wav.replace('.wav','.txt')}\"\n",
    "        \n",
    "        #Reading transcripts and pairs. Note the forced resampling in librosa.load()\n",
    "        array,sampling_rate=librosa.load(wav_path,sr=16000)\n",
    "        transcript=open(transcript_path).read()\n",
    "        \n",
    "        data_dict={\n",
    "            'audio': {'array' : array, 'sampling_rate':sampling_rate},\n",
    "            'sentence' : transcript,\n",
    "            'path' : wav,\n",
    "            'gender' : speaker.gender,\n",
    "            'accent' : speaker.accent,\n",
    "            'speaker': speaker.speaker}\n",
    "        \n",
    "        data_list.append(data_dict)\n",
    "\n",
    "df=pd.DataFrame(data_list)\n",
    "#Making Huggingface dataset\n",
    "l2_data=Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c172e6e4-57c9-404d-88dd-2f530b1d0cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wav2vec2-base-100h'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"facebook/wav2vec2-base-100h\"\n",
    "model_path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15bf40f3-61b7-41af-b685-c7d4aa5b4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "blha=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a366a03a-6117-4ba7-899b-fc2195c273da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import re\n",
    "import tqdm\n",
    "import librosa\n",
    "from datasets import Dataset, load_metric\n",
    "import json\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from src.data_util import DataCollatorCTCWithPadding\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fcb61fb-5ae6-4b33-8ff4-d0217c07ebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook/wav2vec2-base-100h'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = Wav2Vec2ForCTC.from_pretrained(\n",
    "#     \"facebook/wav2vec2-base-100h\",\n",
    "#     attention_dropout=0.0,\n",
    "#     hidden_dropout=0.0,\n",
    "#     feat_proj_dropout=0.0,\n",
    "#     mask_time_prob=0.05,\n",
    "#     layerdrop=0.0,\n",
    "#     ctc_loss_reduction=\"mean\",\n",
    "#     pad_token_id=processor.tokenizer.pad_token_id,\n",
    "#     vocab_size=len(processor.tokenizer),\n",
    "\n",
    "model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5d8b07-a830-42bc-aa2e-febf31b8d781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "wer_metric.compute(predictions=[\"Hi\"], references=[\"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e092f5fb-ba16-404c-b5fa-3d22a0cf62a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[-0.10538473 -0.16507171 -0.13236764 ...  0.12238505  0.1155107\\n  0.12516677]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('corpora/l2arctic/l2_arctic_processed-Copy1.csv')\n",
    "l2_data=Dataset.from_pandas(df)\n",
    "l2_data['input_values'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0742010d-9777-4502-840f-31f6fa94af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' TRAINING WITH US ACCENTS '''\n",
    "\n",
    "us_indices = np.where(np.array(l2_data['accent'])=='US')[0]\n",
    "test_indices_accent= np.where(np.array(l2_data['accent'])!='US')[0]\n",
    "\n",
    "#FOR TRAINING WE SPLIT THE DATA BY UTTERANCES FIRST (EACH SPEAKER DUPLICATES THE DATA)\n",
    "\n",
    "#SELECTING UTTERANCES FOR SPLITS\n",
    "US_Data=l2_data.select(us_indices)\n",
    "utterance_ids=np.unique(US_Data['path'])\n",
    "np.random.shuffle(utterance_ids)\n",
    "train_utterances,val_utterances,test_utterances= np.split(utterance_ids,[int(0.7*len(utterance_ids)),int(0.8*len(utterance_ids))])\n",
    "\n",
    "#COMPILING DATA USING UTTERANCES and ACCENT\n",
    "train_indices=np.where((np.isin(l2_data['path'],train_utterances)) & (np.array(l2_data['accent'])=='US'))[0]\n",
    "val_indices=np.where((np.isin(l2_data['path'],val_utterances)) & (np.array(l2_data['accent'])=='US'))[0]\n",
    "test_indices_us=np.where((np.isin(l2_data['path'],test_utterances)) & (np.array(l2_data['accent'])=='US'))[0]\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(val_indices)\n",
    "np.random.shuffle(test_indices_us)\n",
    "\n",
    "# np.random.shuffle(indices)\n",
    "# train_indices,val_indices,test_indices_us= np.split(indices,[int(0.8*len(indices)),int(0.9*len(indices))])\n",
    "\n",
    "training_dataset = l2_data.select(train_indices)\n",
    "\n",
    "validation_dataset = l2_data.select(val_indices)\n",
    "\n",
    "test_indices=np.concatenate([test_indices_accent,test_indices_us])\n",
    "testing_dataset = l2_data.select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4667d588-de10-40a5-bb2b-87d7e5df98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17G\tcorpora/l2arctic\n"
     ]
    }
   ],
   "source": [
    "!du -hs corpora/l2arctic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3a643e-17da-4194-ace0-b2776e9cd7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Tried reading schema message, was null or length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 2\u001b[0m l2_data\u001b[38;5;241m=\u001b[39m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpora/l2arctic/l2_arctic_processed.hf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/load.py:1755\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory)\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(Path(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)\u001b[38;5;241m.\u001b[39mas_posix()):\n\u001b[0;32m-> 1755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(Path(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASETDICT_JSON_FILENAME)\u001b[38;5;241m.\u001b[39mas_posix()):\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, fs, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory)\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/arrow_dataset.py:1114\u001b[0m, in \u001b[0;36mDataset.load_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory)\u001b[0m\n\u001b[1;32m   1112\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(dataset_size)\n\u001b[1;32m   1113\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[0;32m-> 1114\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_tables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_data_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m split \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1120\u001b[0m split \u001b[38;5;241m=\u001b[39m Split(split) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m split\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/table.py:887\u001b[0m, in \u001b[0;36mconcat_tables\u001b[0;34m(tables, axis)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_tables\u001b[39m(tables: List[Table], axis: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Table:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m    Concatenate tables.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m            Otherwise if there's only one table, it is returned as is.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tables[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/arrow_dataset.py:1115\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1112\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(dataset_size)\n\u001b[1;32m   1113\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[1;32m   1114\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m concat_tables(\n\u001b[0;32m-> 1115\u001b[0m     \u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_file \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_data_files\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1117\u001b[0m )\n\u001b[1;32m   1119\u001b[0m split \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1120\u001b[0m split \u001b[38;5;241m=\u001b[39m Split(split) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m split\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/table.py:478\u001b[0m, in \u001b[0;36mMemoryMappedTable.from_file\u001b[0;34m(cls, filename, replays)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, replays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 478\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_mapped_arrow_table_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_replays(table, replays)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(table, filename, replays)\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/datasets/table.py:51\u001b[0m, in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_mapped_arrow_table_from_file\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[1;32m     50\u001b[0m     memory_mapped_stream \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mmemory_map(filename)\n\u001b[0;32m---> 51\u001b[0m     opened_stream \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_mapped_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m opened_stream\u001b[38;5;241m.\u001b[39mread_all()\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/pyarrow/ipc.py:180\u001b[0m, in \u001b[0;36mopen_stream\u001b[0;34m(source, options, memory_pool)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_stream\u001b[39m(source, \u001b[38;5;241m*\u001b[39m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, memory_pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    Create reader for Arrow streaming format.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    reader : RecordBatchStreamReader\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRecordBatchStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/pyarrow/ipc.py:51\u001b[0m, in \u001b[0;36mRecordBatchStreamReader.__init__\u001b[0;34m(self, source, options, memory_pool)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, \u001b[38;5;241m*\u001b[39m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, memory_pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     options \u001b[38;5;241m=\u001b[39m _ensure_default_ipc_read_options(options)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/pyarrow/ipc.pxi:800\u001b[0m, in \u001b[0;36mpyarrow.lib._RecordBatchStreamReader._open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Tried reading schema message, was null or length 0"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "l2_data=load_from_disk('corpora/l2arctic/l2_arctic_processed.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72743a92-20c3-4cf1-ba78-34e8061ab573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26868, 26872, 26874, ..., 31391, 31392, 31393])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((np.isin(l2_data['path'],train_utterances)) & (np.array(l2_data['accent'])=='US'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54145a5e-5d22-4ea5-b929-b8d228fc7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(np.array(l2_data['accent'])=='US')[0]\n",
    "test_indices_accent= np.where(np.array(l2_data['accent'])!='US')[0]\n",
    "\n",
    "#FOR TRAINING WE SPLIT THE DATA BY UTTERANCES FIRST (EACH SPEAKER DUPLICATES THE DATA)\n",
    "\n",
    "#SELECTING UTTERANCES FOR SPLITS\n",
    "US_Data=l2_data.select(indices)\n",
    "utterance_ids=np.unique(US_Data['path'])\n",
    "np.random.shuffle(utterance_ids)\n",
    "train_utterances,val_utterances,test_utterances= np.split(utterance_ids,[int(0.7*len(utterance_ids)),int(0.8*len(utterance_ids))])\n",
    "\n",
    "#COMPILING DATA FOR UTTERANCES\n",
    "train_indices=np.where(np.isin(US_Data['path'],train_utterances))[0]\n",
    "val_indices=np.where(np.isin(US_Data['path'],val_utterances))[0]\n",
    "test_indices_us=np.where(np.isin(US_Data['path'],test_utterances))[0]\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(val_indices)\n",
    "np.random.shuffle(test_indices_us)\n",
    "\n",
    "\n",
    "training_dataset = l2_data.select(train_indices)\n",
    "\n",
    "validation_dataset = l2_data.select(val_indices)\n",
    "\n",
    "test_indices=np.concatenate([test_indices_accent,test_indices_us])\n",
    "testing_dataset = l2_data.select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae022bb-b823-4dd9-9c5b-7cd98041454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4214c8ba-9983-4508-b3a8-9401f0ead4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabic']\n",
      "['US']\n"
     ]
    }
   ],
   "source": [
    "test_accents=np.unique(testing_dataset['accent'])\n",
    "for accent in test_accents:\n",
    "    accent_indices=np.where(np.array(testing_dataset['accent'])==accent)[0]\n",
    "    accent_test_dataset=testing_dataset.select(accent_indices)\n",
    "    print(np.unique(accent_test_dataset['accent']))\n",
    "#     accent_test_metrics=trainer.evaluate(accent_test_dataset,metric_key_prefix=accent)\n",
    "#     print(accent_test_metrics)\n",
    "#     wandb.log({f'{accent}_wer':accent_test_metrics[f'{accent}_wer']})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f76e698-5617-44b8-8d4e-7ff6f68877d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature` to this method that includes input_values, but you provided ['input_features']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:123\u001b[0m, in \u001b[0;36mWav2Vec2Processor.pad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/transformers/feature_extraction_sequence_utils.py:132\u001b[0m, in \u001b[0;36mSequenceFeatureExtractor.pad\u001b[0;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_values`, has be passed for padding\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_features:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to this method that includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(processed_features\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    138\u001b[0m required_input \u001b[38;5;241m=\u001b[39m processed_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    139\u001b[0m return_attention_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    140\u001b[0m     return_attention_mask \u001b[38;5;28;01mif\u001b[39;00m return_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_attention_mask\n\u001b[1;32m    141\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an instance of `transformers.BatchFeature` or list of `transformers.BatchFeature` to this method that includes input_values, but you provided ['input_features']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1547\n"
     ]
    }
   ],
   "source": [
    "processor.pad([{'input_features':np.array([1,1,1])},{'input_features':[1,1,1]}],return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a8061e-c68f-4edc-9047-9a9ee406a3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1925: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "''' TRAINING WITH US ACCENTS '''\n",
    "indices = np.where(np.array(l2_data['accent'])=='US')[0]\n",
    "test_indices= np.where(np.array(l2_data['accent'])!='US')[0]\n",
    "\n",
    "#Splitting Val from train\n",
    "np.random.shuffle(indices)\n",
    "train_indices,val_indices= np.split(indices,[int(0.8*len(indices))])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding=True)\n",
    "\n",
    "training_dataset = l2_data.select(train_indices)\n",
    "\n",
    "validation_dataset = l2_data.select(val_indices)\n",
    "\n",
    "testing_dataset = l2_data.select(test_indices)\n",
    "\n",
    "\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer*100}\n",
    "\n",
    "\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ").to('cpu')\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='l2_arctic_train/',\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=100,\n",
    "    gradient_checkpointing=True,\n",
    "#    fp16=True,\n",
    "    save_steps=400,\n",
    "    eval_steps=400,\n",
    "    logging_steps=400,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model = 'eval_wer',\n",
    "    report_to='wandb',\n",
    "    seed=123,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3334a3e9-7ff8-41c1-b0f8-4b496a9b525a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cb86c60ce14bf4b71448406fddab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "#validation_dataset = validation_dataset.map(prepare_dataset, remove_columns=['audio','sentence'])\n",
    "testing_dataset= testing_dataset.map(prepare_dataset, remove_columns=['audio','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ebf08ab-0535-41f1-ac50-1f248aae14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader=torch.utils.data.DataLoader(validation_dataset,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4487d13-fd7a-436f-89e9-c08a72fbdb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/akrishnan/miniconda3/envs/coref/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x=trainer.evaluate(validation_dataset.select(range(10)),metric_key_prefix='Vietnamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce5eaebd-3118-429d-8df2-b1638fac1d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arabic_loss': 10.014276504516602, 'Arabic_wer': 100.0, 'Arabic_runtime': 5.9374, 'Arabic_samples_per_second': 1.684, 'Arabic_steps_per_second': 0.337}\n",
      "{'Chinese_loss': 9.919207572937012, 'Chinese_wer': 100.0, 'Chinese_runtime': 6.6148, 'Chinese_samples_per_second': 1.512, 'Chinese_steps_per_second': 0.302}\n"
     ]
    }
   ],
   "source": [
    "test_accents=np.unique(testing_dataset['accent'])\n",
    "for accent in test_accents:\n",
    "    accent_indices=np.where(np.array(testing_dataset['accent'])==accent)[0]\n",
    "    accent_test_dataset=testing_dataset.select(accent_indices[:10])\n",
    "    accent_test_metrics=trainer.evaluate(accent_test_dataset,metric_key_prefix=accent)\n",
    "    print(accent_test_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c6e02-925b-487f-bddc-7c5dad67b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c13e2-284c-4084-8853-e6072f9be829",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TRAINING WITH US ACCENTS '''\n",
    "indices = np.where(np.array(l2_data['accent'])=='US')[0]\n",
    "test_indices= np.where(np.array(l2_data['accent'])!='US')[0]\n",
    "\n",
    "#Splitting Val from train\n",
    "np.random.shuffle(indices)\n",
    "train_indices,val_indices= np.split(indices,[int(0.8*len(indices))])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding=True)\n",
    "\n",
    "training_dataset = l2_data.select(train_indices)\n",
    "\n",
    "validation_dataset = l2_data.select(val_indices)\n",
    "\n",
    "testing_dataset = l2_data.select(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631820d2-1359-460c-a909-ce0621dae9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(l2_data['accent'])=='US')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d4d25-2e17-410f-95cb-70b96efaa8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer*100}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df673182-67e2-478b-b496-8064a8775066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc5dc7ac-82cf-4ddf-9792-b630306e14a0",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6f118-454c-4b30-87e5-4a50db72ee7c",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ea014-fbaa-49fd-bd5e-6625f8e4e5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\\\%\\\\\\\\']'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "    return batch\n",
    "l2_data = l2_data.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b84cbf-3156-4229-8673-d5b5f042a0f5",
   "metadata": {},
   "source": [
    "### Vocab Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fbc6a-91f1-4732-9f11-5bd220dc2dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "vocab = l2_data.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=l2_data.column_names)\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab['vocab'][0]))}\n",
    "\n",
    "#Accounting for blanks and unknowns\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)\n",
    "\n",
    "#Saving into file\n",
    "with open(f'{root_dir}/vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53cd60-1f15-4a2b-bd99-2f294de4257b",
   "metadata": {},
   "source": [
    "# Setting up audio processors and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c61cfe-d9c7-43e5-89d1-948eee41de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13013cd-57b6-4845-b9ce-f719ed27c438",
   "metadata": {},
   "source": [
    "# Pusing the audio and transcript through wav2vec tokenizers and processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2a179-ec0d-4fa1-a178-d5e15e4250de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "l2_data = l2_data.map(prepare_dataset, remove_columns=['audio','sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccc58cd-aa84-42b6-9f1e-7fccf80e935e",
   "metadata": {},
   "source": [
    "# Defining Train Val Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05937a-5e70-47db-a036-dd527591b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(l2_data))\n",
    "np.random.shuffle(indices)\n",
    "train_indices,val_indices,test_indices = np.split(indices,[int(0.7*len(indices)),int(0.8*len(indices))])\n",
    "\n",
    "BATCH_SIZE=8\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding=True)\n",
    "\n",
    "training_dataset = l2_data.select(train_indices)\n",
    "\n",
    "validation_dataset = l2_data.select(val_indices)\n",
    "\n",
    "testing_dataset = l2_data.select(test_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada98fb-f553-476f-85d2-f5c4f10f374d",
   "metadata": {},
   "source": [
    "# Setting up evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c0ed4-d8cf-4b96-a309-c18ad3f76a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d9d0-bd3e-4e1a-95bc-bce4e8702529",
   "metadata": {},
   "source": [
    "# Loading Model, setting up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c579dc7-4367-4d40-9e29-7a22d586087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\",\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66e160-4bcd-4a26-b40f-2c8a7871fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='l2_arctic_train/',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ce415-cee4-42f1-a6fd-01f70ae25a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c122aa9-6b86-4b55-b43e-34d66e90f309",
   "metadata": {},
   "source": [
    "## US-EN Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4c0b3-db5f-48d3-b634-1574575c31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import tqdm\n",
    "# root_dir = '/data/users/akrishnan/multilingual_asr/corpora/l2arctic/cmu_us_en/'\n",
    "# for speaker in tqdm.tqdm(['BDL','CLB','RMS','SLT']):\n",
    "#     old_file_dir=f'{root_dir}/cmu_us_{speaker.lower()}_arctic/wav/'\n",
    "#     transcripts={i[1:-1].strip().split()[0]:\" \".join(i[1:-1].strip().split()[1:])[1:-1] for i in open(f'{root_dir}/cmu_us_{speaker.lower()}_arctic/etc/txt.done.data').read().splitlines()}\n",
    "#     for old_file in tqdm.tqdm_notebook(os.listdir(old_file_dir)):\n",
    "#         old_wav_path=f'{old_file_dir}/{old_file}'\n",
    "#         new_wav_path=f'{root_dir}/{speaker}/wav/{old_file}'\n",
    "#         new_transcript_path=f\"{root_dir}/{speaker}/transcripts/{old_file.replace('.wav','.txt')}\"\n",
    "#         try: \n",
    "#             x=transcripts[old_file.replace('.wav','')]\n",
    "#         except:\n",
    "#             print(f'Transcription for {old_file} not found')\n",
    "#             continue\n",
    "            \n",
    "#         shutil.copyfile(src=old_wav_path, dst=new_wav_path)\n",
    "#         with open(new_transcript_path,'w') as f:\n",
    "#             f.write(transcripts[old_file.replace('.wav','')])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a13641-959a-4eb4-8a1c-12dc505154cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.ones(10)\n",
    "y=np.zeros(20)\n",
    "np.concatenate([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49bf64f-bf6d-478a-a127-6f68580fe4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez('blah.npz',z=x,y=y)\n",
    "np.load('blah.npz')['z']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
